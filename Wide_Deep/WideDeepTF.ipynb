{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Learning - TensorFlow Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bridge\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) \n",
    "\n",
    "print(\"Using TensorFlow version %s\" % (tf.__version__)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input function configured\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 70\n",
    "\n",
    "def generate_input_fn(filename, num_epochs=3, shuffle=True, batch_size=BATCH_SIZE):\n",
    "    df = pd.read_csv(filename, header=None, names=COLUMNS)\n",
    "    labels = df[\"reordered\"].astype(int) \n",
    "    del df[\"reordered\"] # Labels column, already saved to labels variable\n",
    "    \n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df,\n",
    "        y=labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "print('input function configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // Catergorical base columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns configured\n"
     ]
    }
   ],
   "source": [
    "aisle_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"aisle_id\", hash_bucket_size=100)\n",
    "department_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"department_id\", hash_bucket_size=100)\n",
    "order_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"order_id\", hash_bucket_size=500000)\n",
    "user_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"user_id\", hash_bucket_size=50000)\n",
    "product_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"product_id\", hash_bucket_size=50000)\n",
    "\n",
    "print('Categorical columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // Continuous base columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous columns configured\n"
     ]
    }
   ],
   "source": [
    "user_orders = tf.feature_column.numeric_column(\"user_orders\")\n",
    "order_number = tf.feature_column.numeric_column(\"order_number\")\n",
    "add_to_cart_order = tf.feature_column.numeric_column(\"add_to_cart_order\")\n",
    "days_since_prior_order = tf.feature_column.numeric_column(\"days_since_prior_order\")\n",
    "order_hour_of_day = tf.feature_column.numeric_column(\"order_hour_of_day\")\n",
    "order_dow = tf.feature_column.numeric_column(\"order_dow\")\n",
    "\n",
    "print('Continuous columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations complete\n"
     ]
    }
   ],
   "source": [
    "order_hour_of_day_buckets = tf.feature_column.bucketized_column(\n",
    "    order_hour_of_day, boundaries=[ 0, 6, 12, 18 ])\n",
    "order_day_hour = tf.feature_column.crossed_column(\n",
    "    [\"order_hour_of_day\", \"order_dow\"], hash_bucket_size=int(1e4))\n",
    "\n",
    "print('Transformations complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // Wide columns and deep columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide and deep columns configured\n"
     ]
    }
   ],
   "source": [
    "wide_columns = [order_dow, order_hour_of_day, aisle_id, order_number,\n",
    "      user_id, department_id, order_id, add_to_cart_order, user_orders,\n",
    "      product_id, days_since_prior_order, order_day_hour, order_hour_of_day_buckets]\n",
    "\n",
    "deep_columns = [\n",
    "    tf.feature_column.embedding_column(aisle_id, dimension=30),\n",
    "    tf.feature_column.embedding_column(department_id, dimension=30),\n",
    "    tf.feature_column.embedding_column(order_id, dimension=30),\n",
    "    tf.feature_column.embedding_column(user_id, dimension=30),\n",
    "    tf.feature_column.embedding_column(product_id, dimension=30),\n",
    "]\n",
    "\n",
    "print('wide and deep columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory = models/model_WIDE_AND_DEEP_1518649039\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'models/model_WIDE_AND_DEEP_1518649039', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000001C1C4710>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "estimator built\n"
     ]
    }
   ],
   "source": [
    "def create_model_dir(model_type):\n",
    "    return 'models/model_' + model_type + '_' + str(int(time.time()))\n",
    "\n",
    "# If new_model=False, pass in the desired model_dir \n",
    "def get_model(model_type, new_model=False, model_dir=None):\n",
    "    if new_model or model_dir is None:\n",
    "        model_dir = create_model_dir(model_type) # Comment out this line to continue training a existing model\n",
    "    print(\"Model directory = %s\" % model_dir)\n",
    "    \n",
    "    m = None\n",
    "    \n",
    "    # Linear Classifier\n",
    "    if model_type == 'WIDE':\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "            model_dir=model_dir, \n",
    "            feature_columns=wide_columns)\n",
    "\n",
    "    # Deep Neural Net Classifier\n",
    "    if model_type == 'DEEP':\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=[200, 150])\n",
    "\n",
    "    # Combined Linear and Deep Classifier\n",
    "    if model_type == 'WIDE_AND_DEEP':\n",
    "        m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "                model_dir=model_dir,\n",
    "                linear_feature_columns=wide_columns,\n",
    "                dnn_feature_columns=deep_columns,\n",
    "                dnn_hidden_units=[200, 150])\n",
    "        \n",
    "    print('estimator built')\n",
    "    \n",
    "    return m, model_dir\n",
    "    \n",
    "MODEL_TYPE = 'WIDE_AND_DEEP'\n",
    "model_dir = create_model_dir(model_type=MODEL_TYPE)\n",
    "m, model_dir = get_model(model_type = MODEL_TYPE, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"user_id\", \"order_id\", \"product_id\", \"aisle_id\", \"department_id\"]\n",
    "\n",
    "\n",
    "COLUMNS = [\"user_id\", \"user_orders\", \"order_id\", \"order_number\", \"order_dow\", \"order_hour_of_day\", \n",
    "           \"days_since_prior_order\", \"product_id\", \"add_to_cart_order\", \"reordered\", \"aisle_id\", \"department_id\"]\n",
    "\n",
    "FEATURE_COLUMNS = [\"user_id\", \"user_orders\", \"order_id\", \"order_number\", \"order_dow\", \"order_hour_of_day\", \n",
    "           \"days_since_prior_order\", \"product_id\", \"add_to_cart_order\", \"aisle_id\", \"department_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP_1518649039\\model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into models/model_WIDE_AND_DEEP_1518649039\\model.ckpt.\n",
      "INFO:tensorflow:loss = 25.852764, step = 201\n",
      "INFO:tensorflow:global_step/sec: 104.597\n",
      "INFO:tensorflow:loss = 26.685793, step = 301 (0.960 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into models/model_WIDE_AND_DEEP_1518649039\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 18.131187.\n",
      "training done\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_file = \"train53w6.csv\"\n",
    "\n",
    "m.train(input_fn=generate_input_fn(train_file), steps=200)\n",
    "\n",
    "print('training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-14-23:01:01\n",
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP_1518649039\\model.ckpt-400\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-14-23:01:04\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.8387143, accuracy_baseline = 0.8387143, auc = 0.57192904, auc_precision_recall = 0.8716856, average_loss = 0.49119553, global_step = 400, label/mean = 0.8387143, loss = 34.383686, prediction/mean = 0.92375326\n",
      "evaluate done\n",
      "\n",
      "Accuracy: 0.8387143\n",
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_file  = \"test53w6.csv\"\n",
    "\n",
    "results = m.evaluate(input_fn=generate_input_fn(test_file, num_epochs=3, shuffle=True), steps=100)\n",
    "\n",
    "print('evaluate done')\n",
    "print('\\nAccuracy: %s' % results['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP_1518615108\\model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"models/model_WIDE_AND_DEEP_1518615108\\\\temp-b'1518615455'\\\\saved_model.pb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'models/model_WIDE_AND_DEEP_1518615108\\\\1518615455'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def column_to_dtype(column):\n",
    "    if column in CATEGORICAL_COLUMNS:\n",
    "        return tf.string\n",
    "    else:\n",
    "        return tf.float32\n",
    "    \n",
    "feature_spec = {\n",
    "    column: tf.FixedLenFeature(shape=[1], dtype=column_to_dtype(column))\n",
    "        for column in FEATURE_COLUMNS\n",
    "}\n",
    "serving_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
    "m.export_savedmodel(export_dir_base=model_dir,# + '/export/', \n",
    "                            serving_input_receiver_fn=serving_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
