{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files ...\n",
      "done loading\n",
      "merge prior and orders and keep train separate ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfolder = 'F:/rs/Recommender_DNN/input/'\n",
    "\n",
    "\n",
    "print('loading files ...')\n",
    "\n",
    "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])\n",
    "\n",
    "print('done loading')\n",
    "\n",
    "\n",
    "print('merge prior and orders and keep train separate ...')\n",
    "\n",
    "orders_products = orders.merge(prior, how = 'inner', on = 'order_id')\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "\n",
    "\n",
    "del prior\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features I ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features I ...')\n",
    "\n",
    "# sort orders and products to get the rank or the reorder frequency\n",
    "prdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n",
    "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n",
    "\n",
    "# getting products ordered first and second times to calculate probability later\n",
    "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n",
    "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n",
    "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n",
    "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n",
    "sub2 = sub2.reset_index().merge(sub1.reset_index())\n",
    "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n",
    "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n",
    "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n",
    "\n",
    "del sub1, sub2, prdss\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features II ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features II ...')\n",
    "\n",
    "# extracting prior information (features) by user\n",
    "users = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n",
    "users['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\n",
    "users['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n",
    "\n",
    "# merging features about users and orders into one dataset\n",
    "us = orders_products.groupby('user_id').size().to_frame('user_total_products')\n",
    "us['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\n",
    "us['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\n",
    "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n",
    "us['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n",
    "\n",
    "# the average basket size of the user\n",
    "users = users.reset_index().merge(us.reset_index())\n",
    "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n",
    "\n",
    "us = orders[orders['eval_set'] != 0]\n",
    "us = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n",
    "users = users.merge(us)\n",
    "\n",
    "del us\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing features and the main data file  ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Finalizing features and the main data file  ...')\n",
    "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n",
    "data = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n",
    "data['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\n",
    "data['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\n",
    "data['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n",
    "data = data.reset_index()\n",
    "\n",
    "#merging previous data with users\n",
    "data = data.merge(prd, on = 'product_id')\n",
    "data = data.merge(users, on = 'user_id')\n",
    "\n",
    "#user/product combination features about the particular order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "data = data.merge(train_orders[['user_id', 'product_id', 'reordered']],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "data = data.merge(products, on = 'product_id')\n",
    "del users,prd,products\n",
    "     #, orders, train_orders\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n",
    "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n",
    "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16,\n",
    "            'prod_reorder_probability' : np.float16,\n",
    "            'user_orders' : np.uint8,\n",
    "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n",
    "            'user_total_products' : np.uint8, \n",
    "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n",
    "            'order_id'  : np.uint32, 'eval_set' : np.uint8,\n",
    "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16,\n",
    "            'up_orders_since_last_order':np.uint8,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8})\n",
    "\n",
    "data['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered)\n",
    "data['reordered']=data['reordered'].astype(np.uint8)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data.merge(orders_products[['user_id', 'product_id',\"order_dow\", \"order_hour_of_day\"]],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "del orders_products\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.3, random_state= 123)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['eval_set'],axis=1)\n",
    "y = df['reordered'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"order_dow\", \"order_hour_of_day\"]\n",
    "CONTINUOUS_COLUMNS = [ \"user_orders\", \"days_since_prior_order\",\"up_orders\",\"up_first_order\",\"up_last_order\",\"up_average_cart_position\",\"prod_orders\",\"prod_reorder_probability\",\"prod_reorder_ratio\",\"user_period\",\"user_distinct_products\",\"user_mean_days_since_prior\",\"user_total_products\", \"user_average_basket\",\"up_order_rate\",\"up_orders_since_last_order\"]\n",
    "EMBEDDING_COLUMNS = [\"user_id\", \"product_id\",\"order_id\",\"aisle_id\",\"department_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalising the feature columns\n",
    "df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-hot encoding categorical columns\n",
    "df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper to index columns before embeddings\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Keras layer to create Embeddings\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layers for continuous vectors to the deep network\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting datasets into train and test\n",
    "df.reset_index()\n",
    "gc.collect()\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7291026/7291026 [==============================] - 556s 76us/step - loss: 0.1029 - acc: 0.9726\n",
      "2430342/2430342 [==============================] - 125s 52us/step\n",
      "\n",
      " [0.018693832841810188, 0.9999658484279167]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# simply connecting the features to an output layer\n",
    "wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "x = Dropout(0.2)(wide_inp)\n",
    "wide_out = Dense(1, activation='sigmoid')(x)\n",
    "wide = Model(wide_inp, wide_out)\n",
    "wide.compile(Adam(0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide.fit(X_train, y_train, epochs=1,batch_size =64)\n",
    "results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# serialize linear model to JSON\n",
    "wide_json = wide.to_json()\n",
    "with open(\"wide.json\", \"w\") as json_file:\n",
    "    json_file.write(wide_json)\n",
    "# serialize weights to HDF5\n",
    "wide.save_weights(\"wide.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_deep, unique_vals = val2idx(df, EMBEDDING_COLUMNS)\n",
    "X_deep_tr, X_deep_te, y_deep_tr, y_deep_te = train_test_split(df_deep, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining input column for the deep network\n",
    "DEEP_COLNS = EMBEDDING_COLUMNS + CONTINUOUS_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating input dataframe for the merged model\n",
    "X_train_deep = [X_deep_tr[c] for c in DEEP_COLNS]\n",
    "y_train_deep = np.array(y_deep_tr).reshape(-1, 1)\n",
    "X_test_deep = [X_deep_te[c] for c in DEEP_COLNS]\n",
    "y_test_deep = np.array(y_deep_te).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building input tensors for deep network\n",
    "embeddings_tensors = []\n",
    "n_factors = 8\n",
    "reg = 1e-3\n",
    "\n",
    "for ec in EMBEDDING_COLUMNS:\n",
    "    layer_name = ec + '_inp'\n",
    "    t_inp, t_build = embedding_input(\n",
    "    layer_name, unique_vals[ec], n_factors, reg)\n",
    "    embeddings_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)\n",
    "    \n",
    "continuous_tensors = []\n",
    "for cc in CONTINUOUS_COLUMNS:\n",
    "    layer_name = cc + '_in'\n",
    "    t_inp, t_build = continous_input(layer_name)\n",
    "    continuous_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building inputs for deep network\n",
    "deep_inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "deep_inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "deep_inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "deep_inp_embed += [ct[1] for ct in continuous_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2430342/2430342 [==============================] - 1427s 587us/step - loss: 0.4279 - acc: 0.8692\n",
      "810114/810114 [==============================] - 169s 209us/step\n",
      "\n",
      " [0.42280053404732637, 0.8694220813367007]\n"
     ]
    }
   ],
   "source": [
    "#Modeling deep network\n",
    "\n",
    "d = merge(deep_inp_embed, mode='concat')\n",
    "d = Flatten()(d)\n",
    "# 2_. layer to normalise continous columns with the embeddings\n",
    "d = BatchNormalization()(d)\n",
    "d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "d = Dense(50, activation='relu')(d)\n",
    "\n",
    "deep_out = Dense(y_train_deep.shape[1], activation='sigmoid')(d)\n",
    "deep = Model(deep_inp_layer, deep_out)\n",
    "deep.compile(Adam(0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "deep.fit(X_train_deep, y_train_deep, batch_size=64, epochs=1)\n",
    "results = deep.evaluate(X_test_deep, y_test_deep)\n",
    "\n",
    "print (\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize deep model to JSON\n",
    "deep_json = deep.to_json()\n",
    "with open(\"deep.json\", \"w\") as json_file:\n",
    "    json_file.write(deep_json)\n",
    "# serialize weights to HDF5\n",
    "deep.save_weights(\"deep.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "X_tr_wd = [X_train] + X_train_deep\n",
    "Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "X_te_wd = [X_test] + X_test_deep\n",
    "Y_te_wd = y_test_deep  # wide or deep is the same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2430342/2430342 [==============================] - 725s 298us/step - loss: 0.0894 - acc: 0.9989\n",
      "810114/810114 [==============================] - 177s 218us/step\n",
      "\n",
      " [0.0725073088209857, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# WIDE\n",
    "w = Input(shape=(X_train.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "# WIDE + DEEP\n",
    "wd_inp = concatenate([w, d])\n",
    "x = Dropout(0.2)(wd_inp)\n",
    "wd_out = Dense(Y_tr_wd.shape[1], activation='sigmoid', name='wide_deep')(x)\n",
    "\n",
    "wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "\n",
    "wide_deep.compile(optimizer=Adam(lr=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide_deep.fit(X_tr_wd, Y_tr_wd,nb_epoch=1, batch_size=128)\n",
    "\n",
    "results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "print( \"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "wide_deep_json = wide_deep.to_json()\n",
    "with open(\"wide_deep.json\", \"w\") as json_file:\n",
    "    json_file.write(wide_deep_json)\n",
    "# serialize weights to HDF5\n",
    "wide_deep.save_weights(\"wide_deep.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading pre-trained models\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
