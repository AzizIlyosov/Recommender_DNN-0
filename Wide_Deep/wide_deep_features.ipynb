{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files ...\n",
      "done loading\n",
      "merge prior and orders and keep train separate ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfolder = 'F:/rs/Recommender_DNN/input/'\n",
    "\n",
    "\n",
    "print('loading files ...')\n",
    "\n",
    "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])\n",
    "\n",
    "print('done loading')\n",
    "\n",
    "\n",
    "print('merge prior and orders and keep train separate ...')\n",
    "\n",
    "orders_products = orders.merge(prior, how = 'inner', on = 'order_id')\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "\n",
    "\n",
    "del prior\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features I ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features I ...')\n",
    "\n",
    "# sort orders and products to get the rank or the reorder frequency\n",
    "prdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n",
    "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n",
    "\n",
    "# getting products ordered first and second times to calculate probability later\n",
    "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n",
    "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n",
    "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n",
    "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n",
    "sub2 = sub2.reset_index().merge(sub1.reset_index())\n",
    "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n",
    "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n",
    "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n",
    "\n",
    "del sub1, sub2, prdss\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features II ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features II ...')\n",
    "\n",
    "# extracting prior information (features) by user\n",
    "users = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n",
    "users['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\n",
    "users['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n",
    "\n",
    "# merging features about users and orders into one dataset\n",
    "us = orders_products.groupby('user_id').size().to_frame('user_total_products')\n",
    "us['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\n",
    "us['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\n",
    "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n",
    "us['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n",
    "\n",
    "# the average basket size of the user\n",
    "users = users.reset_index().merge(us.reset_index())\n",
    "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n",
    "\n",
    "us = orders[orders['eval_set'] != 0]\n",
    "us = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n",
    "users = users.merge(us)\n",
    "\n",
    "del us\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing features and the main data file  ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Finalizing features and the main data file  ...')\n",
    "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n",
    "data = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n",
    "data['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\n",
    "data['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\n",
    "data['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n",
    "data = data.reset_index()\n",
    "\n",
    "#merging previous data with users\n",
    "data = data.merge(prd, on = 'product_id')\n",
    "data = data.merge(users, on = 'user_id')\n",
    "\n",
    "#user/product combination features about the particular order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "data = data.merge(train_orders[['user_id', 'product_id', 'reordered']],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "data = data.merge(products, on = 'product_id')\n",
    "\n",
    "     #, orders, train_orders\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n",
    "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n",
    "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16,\n",
    "            'prod_reorder_probability' : np.float16,\n",
    "            'user_orders' : np.uint8,\n",
    "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n",
    "            'user_total_products' : np.uint8, \n",
    "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n",
    "            'order_id'  : np.uint32, 'eval_set' : np.uint8,\n",
    "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16,\n",
    "            'up_orders_since_last_order':np.uint8,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8})\n",
    "\n",
    "data['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered)\n",
    "data['reordered']=data['reordered'].astype(np.uint8)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.merge(orders_products[['user_id', 'product_id',\"order_dow\", \"order_hour_of_day\"]],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "del orders_products\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['eval_set'],axis=1)\n",
    "y = df['reordered'].values\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                       0\n",
       "product_id                    0\n",
       "up_orders                     0\n",
       "up_first_order                0\n",
       "up_last_order                 0\n",
       "up_average_cart_position      0\n",
       "prod_orders                   0\n",
       "prod_reorder_probability      0\n",
       "prod_reorder_ratio            0\n",
       "user_orders                   0\n",
       "user_period                   0\n",
       "user_mean_days_since_prior    0\n",
       "user_total_products           0\n",
       "user_distinct_products        0\n",
       "user_average_basket           0\n",
       "order_id                      0\n",
       "eval_set                      0\n",
       "days_since_prior_order        0\n",
       "up_order_rate                 0\n",
       "up_orders_since_last_order    0\n",
       "reordered                     0\n",
       "aisle_id                      0\n",
       "department_id                 0\n",
       "order_dow                     0\n",
       "order_hour_of_day             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"order_dow\", \"order_hour_of_day\"]\n",
    "CONTINUOUS_COLUMNS = [ \"user_orders\", \"days_since_prior_order\",\"up_orders\",\"up_first_order\",\"up_last_order\",\"up_average_cart_position\",\"prod_orders\",\"prod_reorder_probability\",\"prod_reorder_ratio\",\"user_orders\",\"user_period\",\"user_distinct_products\",\"user_mean_days_since_prior\",\"user_total_products\", \"user_average_basket\",\"up_order_rate\",\"up_orders_since_last_order\"]\n",
    "EMBEDDING_COLUMNS = [\"user_id\", \"product_id\",\"order_id\",\"aisle_id\",\"department_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalising the feature columns\n",
    "df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding categorical columns\n",
    "df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper to index columns before embeddings\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Keras layer to create Embeddings\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layers for continuous vectors to the deep network\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting datasets into train and test\n",
    "df.reset_index()\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2430342, 54)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2430342/2430342 [==============================] - 219s 90us/step - loss: 0.0039 - acc: 0.9992\n",
      "810114/810114 [==============================] - 51s 64us/step\n",
      "\n",
      " [1.0266419343861679e-07, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# simply connecting the features to an output layer\n",
    "wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "wide_out = Dense(1, activation='sigmoid')(wide_inp)\n",
    "wide = Model(wide_inp, wide_out)\n",
    "wide.compile(Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide.fit(X_train, y_train, epoch=1,batch_size =64)\n",
    "results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# serialize linear model to JSON\n",
    "wide_json = wide.to_json()\n",
    "with open(\"wide.json\", \"w\") as json_file:\n",
    "    json_file.write(wide_json)\n",
    "# serialize weights to HDF5\n",
    "wide.save_weights(\"wide.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_deep, unique_vals = val2idx(df, EMBEDDING_COLUMNS)\n",
    "X_deep_tr, X_deep_te, y_deep_tr, y_deep_te = train_test_split(df_deep, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining input column for the deep network\n",
    "DEEP_COLNS = EMBEDDING_COLUMNS + CONTINUOUS_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating input dataframe for the merged model\n",
    "X_train_deep = [X_deep_tr[c] for c in DEEP_COLNS]\n",
    "y_train_deep = np.array(y_deep_tr).reshape(-1, 1)\n",
    "X_test_deep = [X_deep_te[c] for c in DEEP_COLNS]\n",
    "y_test_deep = np.array(y_deep_te).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building input tensors for deep network\n",
    "embeddings_tensors = []\n",
    "n_factors = 8\n",
    "reg = 1e-3\n",
    "\n",
    "for ec in EMBEDDING_COLUMNS:\n",
    "    layer_name = ec + '_inp'\n",
    "    t_inp, t_build = embedding_input(\n",
    "    layer_name, unique_vals[ec], n_factors, reg)\n",
    "    embeddings_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)\n",
    "    \n",
    "continuous_tensors = []\n",
    "for cc in CONTINUOUS_COLUMNS:\n",
    "    layer_name = cc + '_in'\n",
    "    t_inp, t_build = continous_input(layer_name)\n",
    "    continuous_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building inputs for deep network\n",
    "deep_inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "deep_inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "deep_inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "deep_inp_embed += [ct[1] for ct in continuous_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "('The name \"user_orders_in\" is used 2 times in the model. All layer names should be unique. Layer names: ', ['user_id_inp', 'product_id_inp', 'order_id_inp', 'aisle_id_inp', 'department_id_inp', 'user_orders_in', 'days_since_prior_order_in', 'up_orders_in', 'up_first_order_in', 'up_last_order_in', 'up_average_cart_position_in', 'prod_orders_in', 'prod_reorder_probability_in', 'prod_reorder_ratio_in', 'user_orders_in', 'user_period_in', 'user_distinct_products_in', 'user_mean_days_since_prior_in', 'user_total_products_in', 'user_average_basket_in', 'up_order_rate_in', 'up_orders_since_last_order_in', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'reshape_1', 'reshape_2', 'reshape_3', 'reshape_4', 'reshape_5', 'reshape_6', 'reshape_7', 'reshape_8', 'reshape_9', 'reshape_10', 'reshape_11', 'reshape_12', 'reshape_13', 'reshape_14', 'reshape_15', 'reshape_16', 'reshape_17', 'merge_1', 'flatten_1', 'batch_normalization_1', 'dense_2', 'dense_3', 'dense_4'])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-279023f8d863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdeep_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_deep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdeep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep_inp_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_deep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_deep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m   1825\u001b[0m                                    \u001b[1;34m' times in the model. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m                                    \u001b[1;34m'All layer names should be unique. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1827\u001b[1;33m                                    'Layer names: ', all_names)\n\u001b[0m\u001b[0;32m   1828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m         \u001b[1;31m# Layer parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ('The name \"user_orders_in\" is used 2 times in the model. All layer names should be unique. Layer names: ', ['user_id_inp', 'product_id_inp', 'order_id_inp', 'aisle_id_inp', 'department_id_inp', 'user_orders_in', 'days_since_prior_order_in', 'up_orders_in', 'up_first_order_in', 'up_last_order_in', 'up_average_cart_position_in', 'prod_orders_in', 'prod_reorder_probability_in', 'prod_reorder_ratio_in', 'user_orders_in', 'user_period_in', 'user_distinct_products_in', 'user_mean_days_since_prior_in', 'user_total_products_in', 'user_average_basket_in', 'up_order_rate_in', 'up_orders_since_last_order_in', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'reshape_1', 'reshape_2', 'reshape_3', 'reshape_4', 'reshape_5', 'reshape_6', 'reshape_7', 'reshape_8', 'reshape_9', 'reshape_10', 'reshape_11', 'reshape_12', 'reshape_13', 'reshape_14', 'reshape_15', 'reshape_16', 'reshape_17', 'merge_1', 'flatten_1', 'batch_normalization_1', 'dense_2', 'dense_3', 'dense_4'])"
     ]
    }
   ],
   "source": [
    "#Modeling deep network\n",
    "\n",
    "d = merge(deep_inp_embed, mode='concat')\n",
    "d = Flatten()(d)\n",
    "# 2_. layer to normalise continous columns with the embeddings\n",
    "d = BatchNormalization()(d)\n",
    "d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "d = Dense(50, activation='relu')(d)\n",
    "\n",
    "deep_out = Dense(y_train_deep.shape[1], activation='sigmoid')(d)\n",
    "deep = Model(deep_inp_layer, deep_out)\n",
    "deep.compile(Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "deep.fit(X_train_deep, y_train_deep, batch_size=64, nb_epoch=1)\n",
    "results = deep.evaluate(X_test_deep, y_test_deep)\n",
    "\n",
    "print (\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize deep model to JSON\n",
    "deep_json = deep.to_json()\n",
    "with open(\"deep.json\", \"w\") as json_file:\n",
    "    json_file.write(deep_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"deep.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
