{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuji\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files ...\n",
      "done loading\n",
      "merge prior and orders and keep train separate ...\n",
      "Creating features I ...\n",
      "Creating features II ...\n",
      "Finalizing features and the main data file  ...\n",
      " Training and test data for later use in F1 optimization and training  ...\n"
     ]
    }
   ],
   "source": [
    "myfolder = 'F:/rs/Recommender_DNN/input/'\n",
    "\n",
    "\n",
    "print('loading files ...')\n",
    "\n",
    "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])\n",
    "\n",
    "print('done loading')\n",
    "\n",
    "\n",
    "print('merge prior and orders and keep train separate ...')\n",
    "\n",
    "orders_products = orders.merge(prior, how = 'inner', on = 'order_id')\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "\n",
    "\n",
    "del prior\n",
    "gc.collect()\n",
    "\n",
    "print('Creating features I ...')\n",
    "\n",
    "# sort orders and products to get the rank or the reorder frequency\n",
    "prdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n",
    "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n",
    "\n",
    "# getting products ordered first and second times to calculate probability later\n",
    "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n",
    "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n",
    "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n",
    "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n",
    "sub2 = sub2.reset_index().merge(sub1.reset_index())\n",
    "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n",
    "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n",
    "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n",
    "\n",
    "del sub1, sub2, prdss\n",
    "gc.collect()\n",
    "\n",
    "print('Creating features II ...')\n",
    "\n",
    "# extracting prior information (features) by user\n",
    "users = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n",
    "users['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\n",
    "users['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n",
    "\n",
    "# merging features about users and orders into one dataset\n",
    "us = orders_products.groupby('user_id').size().to_frame('user_total_products')\n",
    "us['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\n",
    "us['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\n",
    "us['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\n",
    "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n",
    "us['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n",
    "\n",
    "# the average basket size of the user\n",
    "users = users.reset_index().merge(us.reset_index())\n",
    "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n",
    "\n",
    "us = orders[orders['eval_set'] != 0]\n",
    "us = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n",
    "users = users.merge(us)\n",
    "\n",
    "del us\n",
    "gc.collect()\n",
    "\n",
    "print('Finalizing features and the main data file  ...')\n",
    "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n",
    "data = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n",
    "data['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\n",
    "data['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\n",
    "data['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n",
    "data = data.reset_index()\n",
    "\n",
    "#merging previous data with users\n",
    "data = data.merge(prd, on = 'product_id')\n",
    "data = data.merge(users, on = 'user_id')\n",
    "\n",
    "#user/product combination features about the particular order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "data = data.merge(train_orders[['user_id', 'product_id', 'reordered']],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "data = data.merge(products, on = 'product_id')\n",
    "\n",
    "del orders_products     #, orders, train_orders\n",
    "gc.collect()\n",
    "\n",
    "print(' Training and test data for later use in F1 optimization and training  ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting dtypes for data ...\n",
      "Preparing Train and Test sets ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the actual reordered products of the train set in a list format and then delete the original frames\n",
    "train_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\n",
    "orders.set_index('order_id', drop=False, inplace=True)\n",
    "train1=orders[['order_id','eval_set']].loc[orders['eval_set']==1]\n",
    "train1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\n",
    "train1['actual']=train1['actual'].fillna('')\n",
    "n_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n",
    "\n",
    "test1=orders[['order_id','eval_set']].loc[orders['eval_set']==2]\n",
    "test1['actual']=' '\n",
    "traintest1=pd.concat([train1,test1])\n",
    "traintest1.set_index('order_id', drop=False, inplace=True)\n",
    "\n",
    "del orders, train_orders, train1, test1\n",
    "gc.collect()\n",
    "\n",
    "print('setting dtypes for data ...')\n",
    "\n",
    "#reduce the size by setting data types\n",
    "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n",
    "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n",
    "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16,\n",
    "            'prod_reorder_probability' : np.float16,\n",
    "            #'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n",
    "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n",
    "            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16,\n",
    "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n",
    "            'order_id'  : np.uint32, 'eval_set' : np.uint8,\n",
    "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16,\n",
    "            'up_orders_since_last_order':np.uint8,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8})\n",
    "\n",
    "data['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered)\n",
    "data['reordered']=data['reordered'].astype(np.uint8)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('Preparing Train and Test sets ...')\n",
    "\n",
    "# filter by eval_set (train=1, test=2) and dropp the id's columns (not part of training features)\n",
    "# but keep prod_id and user_id in test\n",
    "\n",
    "train = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\n",
    "test =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n",
    "\n",
    "check =  data.drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n",
    "\n",
    "del data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing X,y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['aisle_id', 'days_since_prior_order', 'department_id', 'prod_orders',\n",
       "       'prod_reorder_probability', 'prod_reorder_ratio',\n",
       "       'up_average_cart_position', 'up_first_order', 'up_last_order',\n",
       "       'up_order_rate', 'up_orders', 'up_orders_since_last_order',\n",
       "       'user_average_basket', 'user_distinct_products',\n",
       "       'user_mean_days_since_prior', 'user_orders', 'user_period',\n",
       "       'user_total_products'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('preparing X,y')\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    train[train.columns.difference(['reordered'])], train['reordered'], test_size=0.5, random_state=2)\n",
    "X_train = X_train.drop(['user_reorder_ratio'], axis=1)\n",
    "X_eval = X_eval.drop(['user_reorder_ratio'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>department_id</th>\n",
       "      <th>prod_orders</th>\n",
       "      <th>prod_reorder_probability</th>\n",
       "      <th>prod_reorder_ratio</th>\n",
       "      <th>up_average_cart_position</th>\n",
       "      <th>up_first_order</th>\n",
       "      <th>up_last_order</th>\n",
       "      <th>up_order_rate</th>\n",
       "      <th>up_orders</th>\n",
       "      <th>up_orders_since_last_order</th>\n",
       "      <th>user_average_basket</th>\n",
       "      <th>user_distinct_products</th>\n",
       "      <th>user_mean_days_since_prior</th>\n",
       "      <th>user_orders</th>\n",
       "      <th>user_period</th>\n",
       "      <th>user_total_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12802529</th>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>488</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.362705</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7402041</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>4890</td>\n",
       "      <td>0.444824</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>112</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10128994</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3145</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.413990</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319609</th>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3219</td>\n",
       "      <td>0.405518</td>\n",
       "      <td>0.597080</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285645</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106016</th>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4183</td>\n",
       "      <td>0.451416</td>\n",
       "      <td>0.569926</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>113</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          aisle_id  days_since_prior_order  department_id  prod_orders  \\\n",
       "12802529        78                      30             19          488   \n",
       "7402041          4                      21              9         4890   \n",
       "10128994        16                       5              4         3145   \n",
       "5319609         31                       6              7         3219   \n",
       "4106016        123                      19              4         4183   \n",
       "\n",
       "          prod_reorder_probability  prod_reorder_ratio  \\\n",
       "12802529                  0.263672            0.362705   \n",
       "7402041                   0.444824            0.606544   \n",
       "10128994                  0.253906            0.413990   \n",
       "5319609                   0.405518            0.597080   \n",
       "4106016                   0.451416            0.569926   \n",
       "\n",
       "          up_average_cart_position  up_first_order  up_last_order  \\\n",
       "12802529                        11               2              2   \n",
       "7402041                         18               9              9   \n",
       "10128994                         2               3             14   \n",
       "5319609                          5               5              7   \n",
       "4106016                          4               1              4   \n",
       "\n",
       "          up_order_rate  up_orders  up_orders_since_last_order  \\\n",
       "12802529       0.199951          1                           3   \n",
       "7402041        0.029419          1                          25   \n",
       "10128994       0.125000          2                           2   \n",
       "5319609        0.285645          2                           0   \n",
       "4106016        0.166626          3                          14   \n",
       "\n",
       "          user_average_basket  user_distinct_products  \\\n",
       "12802529                    7                      33   \n",
       "7402041                    26                      72   \n",
       "10128994                   11                      88   \n",
       "5319609                    16                      76   \n",
       "4106016                     7                      31   \n",
       "\n",
       "          user_mean_days_since_prior  user_orders  user_period  \\\n",
       "12802529                          25            5          128   \n",
       "7402041                           10           34          112   \n",
       "10128994                          16           16           12   \n",
       "5319609                           14            7          100   \n",
       "4106016                           20           18          113   \n",
       "\n",
       "          user_total_products  \n",
       "12802529                   39  \n",
       "7402041                   138  \n",
       "10128994                  179  \n",
       "5319609                   114  \n",
       "4106016                   141  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d9a8846369d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"model_feature.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"wide_deep_weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muncompiled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \"\"\"\n\u001b[1;32m--> 348\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "model = model_from_json(myfolder + \"model_feature.json\")\n",
    "model.load_weights(myfolder + \"wide_deep_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wide = Sequential()\n",
    "wide.add(Dense(1, input_dim=X_train.shape[1]))\n",
    "\n",
    "#Deep Neural Network with 2 hidden layers of 100 and 50 neurons\n",
    "deep = Sequential()\n",
    "# TODO: add embedding\n",
    "deep.add(Dense(input_dim=X_train.shape[1], output_dim=100, activation='relu'))\n",
    "deep.add(Dense(100, activation='relu'))\n",
    "deep.add(Dense(50, activation='relu'))\n",
    "deep.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Wide + Deep\n",
    "model = Sequential()\n",
    "model.add(Merge([wide, deep], mode='concat', concat_axis=1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "t0=time.time()\n",
    "model.fit([X_train, X_train], y_train, epoch=3, batch_size=32)\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "\n",
    "loss, accuracy = model.evaluate([X_eval, X_eval], y_eval)\n",
    "print('\\n', 'test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(myfolder + \"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(myfolder + \"wide_deep_weights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "classes = model.predict_classes([X_eval, X_eval], batch_size=32)\n",
    "from keras.models import load_model\n",
    "classes\n",
    "y_eval\n",
    "result = X_eval\n",
    "result['true_lable'] = y_eval\n",
    "result['pred_lable'] = classes\n",
    "result.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
