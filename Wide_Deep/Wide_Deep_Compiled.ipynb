{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BharathiSrinivasan/anaconda2/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import gc\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input,Embedding,LSTM,Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_orders</th>\n",
       "      <th>order_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1737705</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>7350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1681401</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>2680214</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>3237467</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>7350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>2616505</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>7350</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_orders  order_id  eval_set  order_number  order_dow  \\\n",
       "0       17           40   1737705         0             1          2   \n",
       "1       17           40   1681401         0             2          5   \n",
       "2       17           40   2680214         0             3          3   \n",
       "3       17           40   3237467         0             5          6   \n",
       "4       17           40   2616505         0             6          4   \n",
       "\n",
       "   order_hour_of_day  days_since_prior_order  product_id  add_to_cart_order  \\\n",
       "0                 13                      30        7350                  1   \n",
       "1                 10                       3        7350                  1   \n",
       "2                 10                       5        7350                  1   \n",
       "3                 17                       5        7350                  1   \n",
       "4                 17                       5        7350                  2   \n",
       "\n",
       "   reordered  aisle_id  department_id  \n",
       "0          0       115              7  \n",
       "1          1       115              7  \n",
       "2          1       115              7  \n",
       "3          1       115              7  \n",
       "4          1       115              7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input datasets\n",
    "myfolder = '/Users/BharathiSrinivasan/Documents/HU-MEMS-Sem3/Info_Systems/repo/Recommender_DNN/Data/'\n",
    "big_users = pd.read_csv(myfolder + 'bigtrain35.csv')\n",
    "big_users.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features I ...\n",
      "Creating features II ...\n",
      "Finalizing features and the main data file  ...\n"
     ]
    }
   ],
   "source": [
    "print('Creating features I ...')\n",
    "\n",
    "# sort orders and products to get the rank or the reorder frequency\n",
    "prdss = big_users.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n",
    "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n",
    "\n",
    "# getting products ordered first and second times to calculate probability later\n",
    "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n",
    "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n",
    "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n",
    "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n",
    "sub2 = sub2.reset_index().merge(sub1.reset_index())\n",
    "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n",
    "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n",
    "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n",
    "\n",
    "del sub1, sub2, prdss\n",
    "gc.collect()\n",
    "\n",
    "print('Creating features II ...')\n",
    "\n",
    "# extracting prior information (features) by user\n",
    "users = big_users.groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n",
    "users['user_period'] = big_users.groupby(['user_id'])['days_since_prior_order'].sum()\n",
    "users['user_mean_days_since_prior'] = big_users.groupby(['user_id'])['days_since_prior_order'].mean()\n",
    "\n",
    "# merging features about users and orders into one dataset\n",
    "us = big_users.groupby('user_id').size().to_frame('user_total_products')\n",
    "us['eq_1'] = big_users[big_users['reordered'] == 1].groupby('user_id')['product_id'].size()\n",
    "us['gt_1'] = big_users[big_users['order_number'] > 1].groupby('user_id')['product_id'].size()\n",
    "us['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\n",
    "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n",
    "us['user_distinct_products'] = big_users.groupby(['user_id'])['product_id'].nunique()\n",
    "\n",
    "# the average basket size of the user\n",
    "users = users.reset_index().merge(us.reset_index())\n",
    "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n",
    "\n",
    "us = big_users\n",
    "us = us[['user_id', 'order_id', 'days_since_prior_order']]\n",
    "users = users.merge(us)\n",
    "\n",
    "del us\n",
    "gc.collect()\n",
    "\n",
    "print('Finalizing features and the main data file  ...')\n",
    "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n",
    "data = big_users.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n",
    "data['up_first_order'] = big_users.groupby(['user_id', 'product_id'])['order_number'].min()\n",
    "data['up_last_order'] = big_users.groupby(['user_id', 'product_id'])['order_number'].max()\n",
    "data['up_average_cart_position'] = big_users.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n",
    "data = data.reset_index()\n",
    "\n",
    "#merging previous data with users\n",
    "data = data.merge(prd, on = 'product_id')\n",
    "data = data.merge(users, on = 'user_id')\n",
    "\n",
    "#user/product combination features about the particular order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "df = data.merge(big_users[['user_id', 'product_id', 'reordered']],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['eval_set'],axis=1)\n",
    "y = df['reordered'].values\n",
    "df.pop('reordered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"order_dow\", \"order_hour_of_day\"]\n",
    "CONTINUOUS_COLUMNS = [ \"user_orders\", \"order_number\", \"add_to_cart_order\",\"days_since_prior_order\",\"up_orders\",\"up_first_order\",\"up_last_order\",\"up_average_cart_position\",\"prod_orders\",\"prod_reorder_probability\",\"prod_reorder_ratio\",\"user_orders\",\"user_distinct_products\",\"user_average_basket\",\"up_order_rate\"]\n",
    "EMBEDDING_COLUMNS = [\"user_id\", \"product_id\",\"order_id\",\"aisle_id\",\"department_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalising the feature columns\n",
    "df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One-hot encoding categorical columns\n",
    "df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper to index columns before embeddings\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Keras layer to create Embeddings\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layers for continuous vectors to the deep network\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting datasets into train and test\n",
    "df.reset_index()\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simply connecting the features to an output layer\n",
    "wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "wide_out = Dense(1, activation='sigmoid')(wide_inp)\n",
    "wide = Model(wide_inp, wide_out)\n",
    "wide.compile(Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide.fit(X_train, y_train, nb_epoch=1,batch_size =64)\n",
    "results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize linear model to JSON\n",
    "wide_json = wide.to_json()\n",
    "with open(\"wide.json\", \"w\") as json_file:\n",
    "    json_file.write(wide_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"wide.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_deep, unique_vals = val2idx(df, EMBEDDING_COLUMNS)\n",
    "X_deep_tr, X_deep_te, y_deep_tr, y_deep_te = train_test_split(df_deep, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining input column for the deep network\n",
    "DEEP_COLNS = EMBEDDING_COLUMNS + CONTINUOUS_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating input dataframe for the merged model\n",
    "X_train_deep = [X_deep_tr[c] for c in DEEP_COLNS]\n",
    "y_train_deep = np.array(y_deep_tr).reshape(-1, 1)\n",
    "X_test_deep = [X_deep_te[c] for c in DEEP_COLNS]\n",
    "y_test_deep = np.array(y_deep_te).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building input tensors for deep network\n",
    "embeddings_tensors = []\n",
    "n_factors = 8\n",
    "reg = 1e-3\n",
    "\n",
    "for ec in EMBEDDING_COLUMNS:\n",
    "    layer_name = ec + '_inp'\n",
    "    t_inp, t_build = embedding_input(\n",
    "    layer_name, unique_vals[ec], n_factors, reg)\n",
    "    embeddings_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)\n",
    "    \n",
    "continuous_tensors = []\n",
    "for cc in CONTINUOUS_COLUMNS:\n",
    "    layer_name = cc + '_in'\n",
    "    t_inp, t_build = continous_input(layer_name)\n",
    "    continuous_tensors.append((t_inp, t_build))\n",
    "    del(t_inp, t_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building inputs for deep network\n",
    "deep_inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "deep_inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "deep_inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "deep_inp_embed += [ct[1] for ct in continuous_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modeling deep network\n",
    "\n",
    "d = merge(deep_inp_embed, mode='concat')\n",
    "d = Flatten()(d)\n",
    "# 2_. layer to normalise continous columns with the embeddings\n",
    "d = BatchNormalization()(d)\n",
    "d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "d = Dense(50, activation='relu')(d)\n",
    "\n",
    "deep_out = Dense(y_train_deep.shape[1], activation='sigmoid')(d)\n",
    "deep = Model(deep_inp_layer, deep_out)\n",
    "deep.compile(Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "deep.fit(X_train_deep, y_train_deep, batch_size=64, nb_epoch=1)\n",
    "results = deep.evaluate(X_test_deep, y_test_deep)\n",
    "\n",
    "print (\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize deep model to JSON\n",
    "deep_json = deep.to_json()\n",
    "with open(\"deep.json\", \"w\") as json_file:\n",
    "    json_file.write(deep_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"deep.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide + Deep Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "X_tr_wd = [X_train] + X_train_deep\n",
    "Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "X_te_wd = [X_test] + X_test_deep\n",
    "Y_te_wd = y_test_deep  # wide or deep is the same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WIDE\n",
    "w = Input(shape=(X_train.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "# WIDE + DEEP\n",
    "wd_inp = concatenate([w, d])\n",
    "wd_out = Dense(Y_tr_wd.shape[1], activation='sigmoid', name='wide_deep')(wd_inp)\n",
    "\n",
    "wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "\n",
    "wide_deep.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "wide_deep.fit(X_tr_wd, Y_tr_wd, nb_epoch=1, batch_size=128)\n",
    "\n",
    "results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "print( \"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "wide_deep_json = wide_deep.to_json()\n",
    "with open(\"wide_deep.json\", \"w\") as json_file:\n",
    "    json_file.write(wide_deep_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"wide_deep.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading pre-trained models\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
