{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BharathiSrinivasan/anaconda2/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input data\n",
    "data_path = \"/Users/BharathiSrinivasan/Documents/HU-MEMS-Sem3/Info_Systems/repo/InstaCart/input/\"\n",
    "orders = pd.read_csv(data_path + \"orders.csv\")\n",
    "train_orders = pd.read_csv(data_path + \"order_products__train.csv\")\n",
    "prior_orders = pd.read_csv(data_path + \"order_products__prior.csv\")\n",
    "products = pd.read_csv(data_path + \"products.csv\").set_index('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple helper to build the crossed columns in a pandas dataframe\n",
    "def cross_columns(x_cols):\n",
    "    crossed_columns = dict()\n",
    "    colnames = ['_'.join(x_c) for x_c in x_cols]\n",
    "    for cname, x_c in zip(colnames, x_cols):\n",
    "        crossed_columns[cname] = x_c\n",
    "    return crossed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper to index categorical columns before embeddings.\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.iteritems():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.iteritems():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to perform one-hot encoding\n",
    "def onehot(x):\n",
    "    return np.array(OneHotEncoder().fit_transform(x).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper to create Embeddings\n",
    "#To-Do : Do we need biases?\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wide: Linear Model\n",
    "def wide(df_train, df_test, wide_cols, x_cols, target, model_type, method):\n",
    "    \"\"\"Run the wide (linear) model.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    wide_cols   : columns to be used to fit the wide model\n",
    "    x_cols      : columns to be \"crossed\"\n",
    "    target      : the target feature\n",
    "    model_type  : accepts \"wide\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and return the inputs\n",
    "    but NOT run any model.\n",
    "    method      : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"wide\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_wide = pd.concat([df_train, df_test])\n",
    "\n",
    "    crossed_columns_d = cross_columns(x_cols)\n",
    "    categorical_columns = list(\n",
    "        df_wide.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    wide_cols += crossed_columns_d.keys()\n",
    "\n",
    "    for k, v in crossed_columns_d.iteritems():\n",
    "        df_wide[k] = df_wide[v].apply(lambda x: '-'.join(x), axis=1)\n",
    "\n",
    "    df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
    "\n",
    "    dummy_cols = [\n",
    "        c for c in wide_cols if c in categorical_columns + crossed_columns_d.keys()]\n",
    "    df_wide = pd.get_dummies(df_wide, columns=[x for x in dummy_cols])\n",
    "\n",
    "    train = df_wide[df_wide.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_wide[df_wide.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    # make sure all columns are in the same order and life is easier\n",
    "    cols = [target] + [c for c in train.columns if c != target]\n",
    "    train = train[cols]\n",
    "    test = test[cols]\n",
    "\n",
    "    X_train = train.values[:, 1:]\n",
    "    y_train = train.values[:, 0].reshape(-1, 1)\n",
    "    X_test = test.values[:, 1:]\n",
    "    y_test = test.values[:, 0].reshape(-1, 1)\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.fit_transform(X_test)\n",
    "\n",
    "    if model_type == 'wide':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        # metrics parameter needs to be passed as a list or dict\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        # simply connecting the features to an output layer\n",
    "        wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "        w = Dense(y_train.shape[1], activation=activation)(wide_inp)\n",
    "        wide = Model(wide_inp, w)\n",
    "        wide.compile(Adam(0.01), loss=loss, metrics=metrics)\n",
    "        wide.fit(X_train, y_train, nb_epoch=10, batch_size=64)\n",
    "        results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "        print(\"\\n\", results)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep(df_train, df_test, embedding_cols, cont_cols, target, model_type, method):\n",
    "    \"\"\"Run the deep model. Two layers of 100 and 50 neurons. In a decent,\n",
    "    finished code these would be tunable.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    embedding_cols: columns to be passed as embeddings\n",
    "    cont_cols     : numerical columns to be combined with the embeddings\n",
    "    target        : the target feature\n",
    "    model_type    : accepts \"deep\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and returns the inputs\n",
    "    but NOT run any model\n",
    "    method        : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"deep\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    inp_embed, inp_layer: the embedding layers and the input tensors for Model()\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_deep = pd.concat([df_train, df_test])\n",
    "\n",
    "    deep_cols = embedding_cols + cont_cols\n",
    "\n",
    "    # I 'd say that adding numerical columns to embeddings can be done in two ways:\n",
    "    # 1_. normalise the values in the dataframe and pass them to the network\n",
    "    # 2_. add BatchNormalization() layer. (I am not entirely sure this is right)\n",
    "    # I'd say option 1 is the correct one. 2 performs better, which does not say much, but...\n",
    "\n",
    "    # 1_. Scaling in the dataframe\n",
    "    # scaler = MinMaxScaler()\n",
    "    # cont_df = df_deep[cont_cols]\n",
    "    # cont_norm_df = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]))\n",
    "    # cont_norm_df.columns = cont_cols\n",
    "    # for c in cont_cols: df_deep[c] = cont_norm_df[c]\n",
    "\n",
    "    df_deep, unique_vals = val2idx(df_deep, embedding_cols)\n",
    "\n",
    "    train = df_deep[df_deep.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_deep[df_deep.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    embeddings_tensors = []\n",
    "    n_factors = 8\n",
    "    reg = 1e-3\n",
    "    for ec in embedding_cols:\n",
    "        layer_name = ec + '_inp'\n",
    "        t_inp, t_build = embedding_input(\n",
    "            layer_name, unique_vals[ec], n_factors, reg)\n",
    "        embeddings_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    continuous_tensors = []\n",
    "    for cc in cont_cols:\n",
    "        layer_name = cc + '_in'\n",
    "        t_inp, t_build = continous_input(layer_name)\n",
    "        continuous_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    X_train = [train[c] for c in deep_cols]\n",
    "    y_train = np.array(train[target].values).reshape(-1, 1)\n",
    "    X_test = [test[c] for c in deep_cols]\n",
    "    y_test = np.array(test[target].values).reshape(-1, 1)\n",
    "\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "    inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "    inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "    inp_embed += [ct[1] for ct in continuous_tensors]\n",
    "\n",
    "    if model_type == 'deep':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        d = merge(inp_embed, mode='concat')\n",
    "        d = Flatten()(d)\n",
    "        # 2_. layer to normalise continous columns with the embeddings\n",
    "        d = BatchNormalization()(d)\n",
    "        d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "        # d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(50, activation='relu')(d)\n",
    "        # d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(y_train.shape[1], activation=activation)(d)\n",
    "        deep = Model(inp_layer, d)\n",
    "        deep.compile(Adam(0.01), loss=loss, metrics=metrics)\n",
    "        deep.fit(X_train, y_train, batch_size=64, nb_epoch=10)\n",
    "        results = deep.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "        print(\"\\n\", results)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, inp_embed, inp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, method):\n",
    "    \"\"\"Run the wide and deep model. Parameters are the same as those for the\n",
    "    wide and deep functions\n",
    "    \"\"\"\n",
    "\n",
    "    # Default model_type is \"wide_deep\"\n",
    "    X_train_wide, y_train_wide, X_test_wide, y_test_wide = \\\n",
    "        wide(df_train, df_test, wide_cols, x_cols, target, model_type, method)\n",
    "\n",
    "    X_train_deep, y_train_deep, X_test_deep, y_test_deep, deep_inp_embed, deep_inp_layer = \\\n",
    "        deep(df_train, df_test, embedding_cols,cont_cols, target, model_type, method)\n",
    "\n",
    "    X_tr_wd = [X_train_wide] + X_train_deep\n",
    "    Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "    X_te_wd = [X_test_wide] + X_test_deep\n",
    "    Y_te_wd = y_test_deep  # wide or deep is the same here\n",
    "\n",
    "    activation, loss, metrics = fit_param[method]\n",
    "    if metrics: metrics = [metrics]\n",
    "\n",
    "    # WIDE\n",
    "    w = Input(shape=(X_train_wide.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "    # DEEP: the output of the 50 neurons layer will be the deep-side input\n",
    "    d = merge(deep_inp_embed, mode='concat')\n",
    "    d = Flatten()(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "    d = Dense(50, activation='relu', name='deep')(d)\n",
    "\n",
    "    # WIDE + DEEP\n",
    "    wd_inp = concatenate([w, d])\n",
    "    wd_out = Dense(Y_tr_wd.shape[1], activation=activation, name='wide_deep')(wd_inp)\n",
    "    wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "    wide_deep.compile(optimizer=Adam(lr=0.01), loss=loss, metrics=metrics)\n",
    "    wide_deep.fit(X_tr_wd, Y_tr_wd, nb_epoch=10, batch_size=128)\n",
    "\n",
    "    # Maybe you want to schedule a second search with lower learning rate\n",
    "    # wide_deep.optimizer.lr = 0.0001\n",
    "    # wide_deep.fit(X_tr_wd, Y_tr_wd, nb_epoch=10, batch_size=128)\n",
    "\n",
    "    results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "    print (\"\\n\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '_version_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-180a2704c913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '_version_'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
