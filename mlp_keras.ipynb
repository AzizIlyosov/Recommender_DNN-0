{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files ...\n",
      "done loading\n"
     ]
    }
   ],
   "source": [
    "myfolder = 'F:/Dropbox/WS1718/Recommand System/instacart/input/'\n",
    "\n",
    "\n",
    "print('loading files ...')\n",
    "\n",
    "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])\n",
    "\n",
    "print('done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge prior and orders and keep train separate ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('merge prior and orders and keep train separate ...')\n",
    "\n",
    "orders_products = orders.merge(prior, how = 'inner', on = 'order_id')\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "\n",
    "del prior\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features I ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features I ...')\n",
    "\n",
    "# sort orders and products to get the rank or the reorder frequency\n",
    "prdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\n",
    "prdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n",
    "\n",
    "# getting products ordered first and second times to calculate probability later\n",
    "sub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\n",
    "sub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\n",
    "sub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\n",
    "sub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\n",
    "sub2 = sub2.reset_index().merge(sub1.reset_index())\n",
    "sub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\n",
    "sub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\n",
    "prd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n",
    "\n",
    "del sub1, sub2, prdss\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features II ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating features II ...')\n",
    "\n",
    "# extracting prior information (features) by user\n",
    "users = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\n",
    "users['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\n",
    "users['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n",
    "\n",
    "# merging features about users and orders into one dataset\n",
    "us = orders_products.groupby('user_id').size().to_frame('user_total_products')\n",
    "us['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\n",
    "us['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\n",
    "us['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\n",
    "us.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\n",
    "us['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n",
    "\n",
    "# the average basket size of the user\n",
    "users = users.reset_index().merge(us.reset_index())\n",
    "users['user_average_basket'] = users['user_total_products'] / users['user_orders']\n",
    "\n",
    "us = orders[orders['eval_set'] != 0]\n",
    "us = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n",
    "users = users.merge(us)\n",
    "\n",
    "del us\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing features and the main data file  ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Finalizing features and the main data file  ...')\n",
    "# merging orders and products and grouping by user and product and calculating features for the user/product combination\n",
    "data = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\n",
    "data['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\n",
    "data['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\n",
    "data['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\n",
    "data = data.reset_index()\n",
    "\n",
    "#merging previous data with users\n",
    "data = data.merge(prd, on = 'product_id')\n",
    "data = data.merge(users, on = 'user_id')\n",
    "\n",
    "#user/product combination features about the particular order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "data = data.merge(train_orders[['user_id', 'product_id', 'reordered']],\n",
    "                  how = 'left', on = ['user_id', 'product_id'])\n",
    "data = data.merge(products, on = 'product_id')\n",
    "\n",
    "del orders_products     #, orders, train_orders\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training and test data for later use in F1 optimization and training  ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(' Training and test data for later use in F1 optimization and training  ...')\n",
    "\n",
    "#save the actual reordered products of the train set in a list format and then delete the original frames\n",
    "train_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\n",
    "orders.set_index('order_id', drop=False, inplace=True)\n",
    "train1=orders[['order_id','eval_set']].loc[orders['eval_set']==1]\n",
    "train1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\n",
    "train1['actual']=train1['actual'].fillna('')\n",
    "n_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n",
    "\n",
    "test1=orders[['order_id','eval_set']].loc[orders['eval_set']==2]\n",
    "test1['actual']=' '\n",
    "traintest1=pd.concat([train1,test1])\n",
    "traintest1.set_index('order_id', drop=False, inplace=True)\n",
    "\n",
    "del orders, train_orders, train1, test1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting dtypes for data ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('setting dtypes for data ...')\n",
    "\n",
    "#reduce the size by setting data types\n",
    "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n",
    "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n",
    "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16,\n",
    "            'prod_reorder_probability' : np.float16,\n",
    "            #'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n",
    "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n",
    "            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16,\n",
    "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n",
    "            'order_id'  : np.uint32, 'eval_set' : np.uint8,\n",
    "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16,\n",
    "            'up_orders_since_last_order':np.uint8,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8})\n",
    "\n",
    "data['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered)\n",
    "data['reordered']=data['reordered'].astype(np.uint8)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train and Test sets ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Preparing Train and Test sets ...')\n",
    "\n",
    "# filter by eval_set (train=1, test=2) and dropp the id's columns (not part of training features)\n",
    "# but keep prod_id and user_id in test\n",
    "\n",
    "train = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\n",
    "test =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n",
    "\n",
    "check =  data.drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing X,y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('preparing X,y')\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    train[train.columns.difference(['reordered'])], train['reordered'], test_size=0.9, random_state=2)\n",
    "X_train = X_train.drop(['user_reorder_ratio'], axis=1)\n",
    "X_eval = X_eval.drop(['user_reorder_ratio'], axis=1)\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load trained model\n"
     ]
    }
   ],
   "source": [
    "#print('Building Model')\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=18, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='rmsprop',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "#or to load model\n",
    "print('load trained model')\n",
    "model = load_model(myfolder+'model_mlp.h5')\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do not fit it again if you load the model\n",
    "#print('Fitting Model')\n",
    "#model.fit(X_train, y_train,\n",
    " #         epochs=20,\n",
    "  #        batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_eval, y_eval, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(myfolder+'model_mlp.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
